#include <vector>
#include <unordered_map>
#include <random>
#include <ctime>
#include <iostream>
#include <fstream>
#include <numeric>
#include <limits>
#include <algorithm>

// Decision Tree Node class
class Node {
public:
    Node* left;
    Node* right;
    int featureIndex;
    double threshold;
    int predictedClass;
    bool isLeaf;

    Node() : left(nullptr), right(nullptr), featureIndex(-1), threshold(0.0),
             predictedClass(-1), isLeaf(false) {}
};

// Decision Tree Classifier class
class DecisionTree {
private:
    Node* root;
    int maxDepth;
    int maxFeatures;
    std::vector<std::vector<double>> sortedFeatures;

public:
    DecisionTree(int maxDepth, int maxFeatures, Node* root = nullptr) : maxDepth(maxDepth), maxFeatures(maxFeatures), root(root) {}

    // Train decision tree
    void train(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {
        // Features MUST be sorted for this algorithm to work correctly (ascending order)
        root = buildTree(features, labels, 0);
    }

    // Predict class label for a single instance
    int predict(const std::vector<double>& instance) {
        Node* current = root;
        while (current && !current->isLeaf) {
            if (instance[current->featureIndex] <= current->threshold) {
                current = current->left;
            } else {
                current = current->right;
            }
        }
        return current->predictedClass;
    }

    Node* const getRoot() const {
        return this->root;
    }

private:

    // Build decision tree recursively
    Node* buildTree(const std::vector<std::vector<double>>& features, const std::vector<int>& labels, int depth) {
        Node* node = new Node();
        if (depth >= maxDepth || features.empty() || isPure(labels)) {
            node->isLeaf = true;
            if (!labels.empty()) {
                node->predictedClass = getMajorityClass(labels);
            }
        } else {
            std::vector<int> randomFeatureIndex = getRandomFeatures(features[0].size());
            int featureIndex;
            double threshold;
            findBestSplit(features, labels, randomFeatureIndex, featureIndex, threshold);

            node->featureIndex = featureIndex;
            node->threshold = threshold;

            std::vector<std::vector<double>> leftFeatures, rightFeatures;
            std::vector<int> leftLabels, rightLabels;

            // Split data based on threshold
            for (size_t i = 0; i < features.size(); ++i) {
                if (features[i][featureIndex] <= threshold) {
                    leftFeatures.push_back(features[i]);
                    leftLabels.push_back(labels[i]);
                } else {
                    rightFeatures.push_back(features[i]);
                    rightLabels.push_back(labels[i]);
                }
            }

            node->left = buildTree(leftFeatures, leftLabels, depth + 1);
            node->right = buildTree(rightFeatures, rightLabels, depth + 1);
        }
        return node;
    }

    std::vector<int> getRandomFeatures(int num_features) {
        std::vector<int> features(num_features);
        std::iota(features.begin(), features.end(), 0);

        std::random_device rd;
        std::mt19937 g(rd());
        std::shuffle(features.begin(), features.end(), g);

        features.resize(maxFeatures);

        // std::vector<int> randomFeatures;

        // randomFeatures.push_back(features[42]);

        return features;
        // return randomFeatures;
    }

    // Check if all labels in a set are the same
    bool isPure(const std::vector<int>& labels) {
        if (labels.empty()) {
            return true;
        }
        for (size_t i = 1; i < labels.size(); ++i) {
            if (labels[i] != labels[0]) {
                return false;
            }
        }
        return true;
    }

    // Find the best split point based on Gini impurity
    void findBestSplit(const std::vector<std::vector<double>>& features, const std::vector<int>& labels, const std::vector<int>& randomFeatures,
                       int& bestFeature, double& bestThreshold) {
        double bestGini = std::numeric_limits<double>::max();

        for (size_t featureIndex : randomFeatures) {

            std::vector<double> featureValues;
            for (const auto& instance : features) {
                featureValues.push_back(instance[featureIndex]);
            }

            // As data is pre-sorted, the loop can exit early when the best split is found
            for (size_t i = 0; i < featureValues.size() - 1; ++i) {
                double threshold = (featureValues[i] + featureValues[i + 1]) / 2.0;

                std::vector<int> leftLabels, rightLabels;
                std::vector<int> leftCount(2, 0);
                std::vector<int> rightCount(2, 0);
                double giniLeft = 0.0, giniRight = 0.0;

                for (size_t i = 0; i < features.size(); ++i) {
                    if (features[i][featureIndex] <= threshold) {
                        leftLabels.push_back(labels[i]);
                        leftCount[labels[i]]++;
                    } else {
                        rightLabels.push_back(labels[i]);
                        rightCount[labels[i]]++;
                    }
                }

                int numInstances = leftLabels.size() + rightLabels.size();

                if (!leftLabels.empty()) {
                    for (const auto& count : leftCount) {
                        double classProb = static_cast<double>(count) / leftLabels.size();
                        giniLeft += std::pow(classProb, 2);
                    }
                    giniLeft = 1.0 - giniLeft;
                }

                if (!rightLabels.empty()) {
                    for (const auto& count : rightCount) {
                        double classProb = static_cast<double>(count) / rightLabels.size();
                        giniRight += std::pow(classProb, 2);
                    }
                    giniRight = 1.0 - giniRight;
                }

                double gini = (static_cast<double>(leftLabels.size()) / numInstances) * giniLeft +
                              (static_cast<double>(rightLabels.size()) / numInstances) * giniRight;

                if (gini < bestGini) {
                    bestGini = gini;
                    bestFeature = featureIndex;
                    bestThreshold = threshold;
                } else {
                    // If gini doesn't improve, stop trying for this featureIndex
                    break;
                }
            }
        }
    }

    int getMajorityClass(const std::vector<int>& labels) {
        // Important to execution speed
        std::unordered_map<int, int> classCounts;
        int majorityClass = -1;
        int maxCount = 0;

        for (const auto& label : labels) {
            classCounts[label]++;
            if (classCounts[label] > maxCount) {
                maxCount = classCounts[label];
                majorityClass = label;
            }
        }

        return majorityClass;
    }
};

// Random Forest Classifier class
class RandomForest {
private:
    int numTrees;
    int maxDepth;
    int maxFeatures;
    std::vector<DecisionTree*> trees;

public:
    RandomForest() : numTrees(10), maxDepth(5), maxFeatures(1) {}

    RandomForest(const std::string& filename) : numTrees(10), maxDepth(5), maxFeatures(1) {
        loadModel(filename);
    }

    RandomForest(int numTrees, int maxDepth, int maxFeatures) : numTrees(numTrees), maxDepth(maxDepth), maxFeatures(maxFeatures) {}

    // Train random forest
    void train(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {

        for (int i = 0; i < numTrees; ++i) {
            clock_t startTime = clock();

            DecisionTree* tree = new DecisionTree(maxDepth, maxFeatures);

            // TODO: Check if sampling with replacement is better
            // std::vector<std::vector<double>> sampledFeatures;
            // std::vector<int> sampledLabels;

            // Randomly sample features and labels
            // for (size_t j = 0; j < features.size(); ++j) {
            //     int index = randomInt(0, features.size() - 1);
            //     sampledFeatures.push_back(features[index]);
            //     sampledLabels.push_back(labels[index]);
            // }

            // tree->train(sampledFeatures, sampledLabels);

            // Training features and labels MUST be sorted
            tree->train(features, labels);
            trees.push_back(tree);

            clock_t endTime = clock();
            double elapsedTime = double(endTime - startTime) / CLOCKS_PER_SEC * 1000.0;

            std::cout << "Trained tree " << i + 1 << ", " << elapsedTime << " ms" << std::endl;
        }
    }

    int predict(const std::vector<double>& instance) {
        std::vector<int> predictions(trees.size());
        for (size_t i = 0; i < trees.size(); ++i) {
            predictions[i] = trees[i]->predict(instance);
        }
        std::vector<int> classCounts(2, 0);
        for (const auto& prediction : predictions) {
            classCounts[prediction]++;
        }
        int maxCount = -1;
        int predictedClass = -1;
        for (size_t i = 0; i < classCounts.size(); ++i) {
            if (classCounts[i] > maxCount) {
                maxCount = classCounts[i];
                predictedClass = i;
            }
        }

        return predictedClass;
    }

    double calculateAccuracy(const std::vector<std::vector<double>>& testFeatures, const std::vector<int>& testLabels) {
        double correctPredictions = 0;

        for (size_t i = 0; i < testFeatures.size(); ++i) {
            if (this->predict(testFeatures[i]) == testLabels[i]) {
                correctPredictions++;
            }
        }

        // std::cout << "Correct predictions: " << correctPredictions << "/" << testFeatures.size() << std::endl;

        return correctPredictions / testFeatures.size();
    }

    void saveModel(const std::string& filename) {
        std::stringstream stream;

        // Write the model properties
        stream.write(reinterpret_cast<const char*>(&numTrees), sizeof(numTrees));
        stream.write(reinterpret_cast<const char*>(&maxDepth), sizeof(maxDepth));
        stream.write(reinterpret_cast<const char*>(&maxFeatures), sizeof(maxFeatures));

        // Write each decision tree
        for (const auto& tree : trees) {
            saveDecisionTreeToStream(stream, tree->getRoot());
        }

        std::ofstream file(filename, std::ios::binary);
        if (file) {
            std::string serialisedData = stream.str();
            file.write(serialisedData.c_str(), serialisedData.size());
            file.close();
        } else {
            std::cerr << "Failed to open file" << std::endl;
        }
    }

    void loadModel(const std::string& filename) {
        std::ifstream file(filename, std::ios::binary);
        if (!file) {
            std::cerr << "Failed to open the file for reading." << std::endl;
            return;
        }

        std::stringstream stream;
        stream << file.rdbuf();

        // Read the model properties
        stream.read(reinterpret_cast<char*>(&numTrees), sizeof(numTrees));
        stream.read(reinterpret_cast<char*>(&maxDepth), sizeof(maxDepth));
        stream.read(reinterpret_cast<char*>(&maxFeatures), sizeof(maxFeatures));

        // Load each decision tree
        trees.clear();
        for (int i = 0; i < numTrees; ++i) {
            Node* root = loadDecisionTreeFromStream(stream);
            DecisionTree* tree = new DecisionTree(maxDepth, maxFeatures, root);
            trees.push_back(tree);
        }

        file.close();
    }

private:
    // Helper function to save a decision tree recursively
    void saveDecisionTreeToStream(std::ostream& stream, Node* node) {
        if (!node) {
            return;
        }

        // Save node properties
        stream.write(reinterpret_cast<const char*>(&node->featureIndex), sizeof(node->featureIndex));
        stream.write(reinterpret_cast<const char*>(&node->threshold), sizeof(node->threshold));
        stream.write(reinterpret_cast<const char*>(&node->predictedClass), sizeof(node->predictedClass));
        stream.write(reinterpret_cast<const char*>(&node->isLeaf), sizeof(node->isLeaf));

        // Recursively save left and right subtrees
        saveDecisionTreeToStream(stream, node->left);
        saveDecisionTreeToStream(stream, node->right);
    }

    // Helper function to load a decision tree recursively
    Node* loadDecisionTreeFromStream(std::istream& stream) {
        Node* node = new Node();
        // Load node properties
        stream.read(reinterpret_cast<char*>(&node->featureIndex), sizeof(node->featureIndex));
        stream.read(reinterpret_cast<char*>(&node->threshold), sizeof(node->threshold));
        stream.read(reinterpret_cast<char*>(&node->predictedClass), sizeof(node->predictedClass));
        stream.read(reinterpret_cast<char*>(&node->isLeaf), sizeof(node->isLeaf));

        // Recursively load left and right subtrees if they exist
        if (!node->isLeaf) {
            node->left = loadDecisionTreeFromStream(stream);
            node->right = loadDecisionTreeFromStream(stream);
        }

        return node;
    }
};

// Split data into train and test sets
void trainTestSplit(const std::vector<std::vector<double>>& allFeatures, const std::vector<int>& allLabels,
               std::vector<std::vector<double>>& trainFeatures, std::vector<int>& trainLabels,
               std::vector<std::vector<double>>& testFeatures, std::vector<int>& testLabels,
               double trainRatio = 0.8) {
    if (allFeatures.size() != allLabels.size()) {
        std::cerr << "Mismatch between features and labels!" << std::endl;
        return;
    }

    int numTrainInstances = std::round(allFeatures.size() * trainRatio);

    std::vector<int> indices(allFeatures.size());
    std::iota(indices.begin(), indices.end(), 0);
    std::random_device rd;
    std::mt19937 rng(rd());
    std::shuffle(indices.begin(), indices.end(), rng);

    for (int i = 0; i < numTrainInstances; ++i) {
        trainFeatures.push_back(allFeatures[indices[i]]);
        trainLabels.push_back(allLabels[indices[i]]);
    }

    for (int i = numTrainInstances; i < allFeatures.size(); ++i) {
        testFeatures.push_back(allFeatures[indices[i]]);
        testLabels.push_back(allLabels[indices[i]]);
    }
}