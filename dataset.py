import hashlib
import random
import json
import os

from multiprocessing import Pool, cpu_count
from pefile import PE, PEFormatError
from numpy import mean
import pandas as pd

## Change the below variables as necessary ##

USED_CODES = []

BENIGN_FILE_PATHS = ['C:\\', 'D:\\']  # Sourced from local machine

MALWARE_FILE_PATHS = [  # Sourced from VirusShare (https://virusshare.com/)
    'E:\\VirusShare_00459',
    'E:\\VirusShare_00460',
    'E:\\VirusShare_00461',
    'E:\\VirusShare_00462',
    'E:\\VirusShare_00463',
    'E:\\VirusShare_00464',
    'E:\\VirusShare_00465',
    'E:\\VirusShare_00466',
]

# Junctions and folders to ignore when traversing file system
IGNORED_FOLDERS = [
    '$Recycle.Bin',
    'Documents and Settings',
    'Application Data',
    'Cookies',
    'Local Settings',
    'NetHood',
    'PrintHood',
    'Recent',
    'SendTo',
    'Start Menu',
    'Templates',
    'Temporary Internet Files',

    # My additions
    '_Coding',
]

## End of variables ##


def get_attributes(file_path, malware=1, fast_load=False):
    try:
        # When fast_load is set to True, some PE attributes are not parsed
        pe = PE(file_path, fast_load=fast_load)
    except (PEFormatError, AttributeError):
        print('Not a PE file')
        return {}

    arr = {'Name': os.path.basename(file_path), 'Malware': malware}

    dos_header_attrs = [
        'e_magic', 'e_cblp', 'e_cp', 'e_crlc', 'e_cparhdr', 'e_minalloc', 'e_maxalloc',
        'e_ss', 'e_sp', 'e_csum', 'e_ip', 'e_cs', 'e_lfarlc', 'e_ovno', 'e_oemid',
        'e_oeminfo', 'e_lfanew'
    ]
    dos_header_data = {attr: getattr(pe.DOS_HEADER, attr, 0)
                       for attr in dos_header_attrs}
    arr.update(dos_header_data)

    optional_header_attrs = [
        'Magic', 'MajorLinkerVersion', 'MinorLinkerVersion', 'SizeOfCode', 'SizeOfInitializedData',
        'SizeOfUninitializedData', 'AddressOfEntryPoint', 'BaseOfCode', 'ImageBase',
        'SectionAlignment', 'FileAlignment', 'MajorOperatingSystemVersion',
        'MinorOperatingSystemVersion', 'MajorImageVersion', 'MinorImageVersion',
        'MajorSubsystemVersion', 'MinorSubsystemVersion', 'SizeOfImage', 'SizeOfHeaders',
        'CheckSum', 'Subsystem', 'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve',
        'SizeOfHeapCommit', 'LoaderFlags', 'NumberOfRvaAndSizes', 'DllCharacteristics'
    ]
    optional_header_data = {attr: getattr(
        pe.OPTIONAL_HEADER, attr, 0) for attr in optional_header_attrs}
    arr.update(optional_header_data)

    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        arr['SizeOfResources'] = len(pe.DIRECTORY_ENTRY_RESOURCE.entries)
    else:
        arr['SizeOfResources'] = 0

    try:
        arr['SizeOfLoadConfig'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.__sizeof__() if hasattr(pe, 'DIRECTORY_ENTRY_LOAD_CONFIG') else 0
    except AttributeError:
        print('Error accessing DIRECTORY_ENTRY_LOAD_CONFIG')
        arr['SizeOfLoadConfig'] = 0

    arr['SizeOfSections'] = len(getattr(pe, 'sections', []))

    section_attrs = [
        ['RawSize', 'SizeOfRawData'],
        ['PhysicalAddress', 'Misc_PhysicalAddress'],
        ['VirtualSize', 'Misc_VirtualSize'],
        ['VirtualAddress', 'VirtualAddress'],
        ['RawData', 'PointerToRawData'],
        ['Characteristics', 'Characteristics']
    ]

    for key, value in section_attrs:
        temp = [getattr(s, value, [0]) for s in getattr(pe, 'sections', [0])]
        temp = temp if temp else [0]
        arr[f'SectionMin{key}'], arr[f'SectionMax{key}'], arr[f'SectionMean{key}'] = min(temp), max(temp), int(mean(temp))

    entropy = [s.get_entropy() for s in getattr(pe, 'sections', [])]
    arr['SectionMinEntropy'], arr['SectionMaxEntropy'], arr['SectionMeanEntropy'] = (min(entropy), max(entropy), int(mean(entropy))) if entropy else (0, 0, 0)

    arr['NumberOfImports'] = len(getattr(pe, 'DIRECTORY_ENTRY_IMPORT', []))
    arr['SizeOfImports'] = sum(len(x.imports) for x in getattr(pe, 'DIRECTORY_ENTRY_IMPORT', []))

    if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
        arr['NumberOfExports'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
        arr['SizeOfExports'] = sum(x.__sizeof__() for x in pe.DIRECTORY_ENTRY_EXPORT.symbols)
    else:
        arr['NumberOfExports'] = 0
        arr['SizeOfExports'] = 0

    arr['MD5'] = hashlib.md5(pe.__data__).hexdigest()

    return {x: arr[x] for x in sorted(arr)}


def is_compatible(file_path):
    try:
        PE(file_path, fast_load=True)
        return True
    except (PEFormatError, Exception):  # Exception is raised when a PermissionError is encountered
        return False


def get_files(path):
    for element in os.listdir(path):
        if element in IGNORED_FOLDERS:
            continue
        element_path = os.path.join(path, element)
        if os.path.isfile(element_path):
            yield element_path
        elif os.path.isdir(element_path):
            yield from get_files(element_path)


def get_unique_code(used_codes):
    code = random.randint(10000000, 99999999)
    while code in used_codes:
        code = random.randint(10000000, 99999999)
    used_codes.append(code)
    return code


def save_files(save_path, file_paths, custom_parent=None, save_increment=1000):
    files = []
    saved_files = []
    if os.path.exists(save_path):
        with open(save_path, 'r', encoding='utf-8') as f:
            files = json.load(f)
            saved_files = [info[0] for info in files]

    for path in file_paths:
        for i, filepath in enumerate(get_files(path)):
            if filepath not in saved_files and os.path.exists(filepath) and is_compatible(filepath):
                parent = custom_parent if custom_parent else path.split('\\')[1]
                files.append([filepath, parent, get_unique_code(USED_CODES)])
            if i % save_increment == 0 and i != 0:
                with open(save_path, 'w', encoding='utf-8') as f:
                    json.dump(files, f, indent=4)
                print(i, parent)
        print(f'Saved all files in {path}')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(files, f, indent=4)


def process_file(args):
    filepath, parent, code, malware_flag = args
    if os.path.exists(filepath):
        attributes = get_attributes(filepath, malware_flag)
        if attributes:
            return {
                code: {
                    'Code': code,
                    'Parent': parent,
                    **attributes,
                }
            }
    return None


def save_data(save_path, file_path, malware_flag, save_increment=1000, processes=cpu_count()):
    with open(file_path, 'r', encoding='utf-8') as f:
        files = json.load(f)

    data = {}
    if os.path.exists(save_path):
        with open(save_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

    data_codes = set(int(key) for key in data.keys())
    files = [(file[0], file[1], file[2], malware_flag) for file in files if file[2] not in data_codes]
    grouped_files = [files[i:i+save_increment] for i in range(0, len(files), save_increment)]

    for i, file_group in enumerate(grouped_files):
        with Pool(processes=processes) as pool:
            results = pool.map(process_file, file_group)
            results = [result for result in results if result is not None]
            data.update({k: v for result in results for k, v in result.items()})

        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4)

        print(f'{min((i+1)*save_increment, len(files))}/{len(files)}')


def json_to_csv(json_file, csv_file, split_size=10000):
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    chunks = [list(data.values())[i:i+split_size] for i in range(0, len(data), split_size)]

    with Pool() as pool:
        results = pool.map(pd.json_normalize, chunks)

    df = pd.concat(results, ignore_index=True)

    df.to_csv(csv_file, index=False)


if __name__ == '__main__':

    for file in ['benign_files.json', 'malware_files.json']:
        if os.path.exists(file):
            with open(file, 'r', encoding='utf-8') as f:
                USED_CODES.extend([info[2] for info in json.load(f)])

    save_files('benign_files.json', BENIGN_FILE_PATHS, custom_parent='LocalMachine')
    save_files('malware_files.json', MALWARE_FILE_PATHS)

    save_data('benign_data.json', 'benign_files.json', 0)
    save_data('malware_data.json', 'malware_files.json', 1)

    json_to_csv('benign_data.json', 'benign_data.csv')
    json_to_csv('malware_data.json', 'malware_data.csv')
