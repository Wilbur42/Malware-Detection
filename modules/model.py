import time
import sys
import os

from multiprocessing import Pool, Manager
import pickle
import numpy as np
import pandas as pd

def train_test_split(data, labels, test_size=0.2, random_seed=None):
    if random_seed is not None:
        np.random.seed(random_seed)

    num_samples = data.shape[0]
    num_test_samples = int(num_samples * test_size)

    # Shuffle the samples
    indices = np.random.permutation(num_samples)

    # Split the samples into training and testing sets
    test_indices = indices[:num_test_samples]
    train_indices = indices[num_test_samples:]

    # Split the data and labels
    train_data = data[train_indices]
    train_labels = labels[train_indices]
    test_data = data[test_indices]
    test_labels = labels[test_indices]

    return train_data, test_data, train_labels, test_labels

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

class Node:
    # A node in the decision tree
    def __init__(self, feature=None, threshold=None, left=None, right=None, class_label=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.class_label = class_label

class DecisionTreeClassifier:
    def __init__(self, max_depth=None, max_features=None, min_samples_split=2):
        self.root = None
        self.max_features = max_features
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split

    # Train the decision tree
    def fit(self, X, y):
        self.root = self._build_tree(X, y, depth=0)

    # Predict the class labels
    def predict(self, X):
        return [self._traverse_tree(self.root, x) for x in X]

    # Build the decision tree
    def _build_tree(self, X, y, depth):
        # Check the depth of the tree
        if depth == self.max_depth or len(y) < self.min_samples_split:
            # Get the majority class label
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Get indicies of features from the data
        #   (Features - Attributes of each sample)
        features = self._get_features(X.shape[1])

        # Get the best feature and threshold for the current node
        #    (The feature and threshold that best splits the data)
        best_feature, best_threshold = self._get_split(X, y, features)

        # TODO: Feature Importance

        # Check if the current node is a leaf node
        if None in (best_feature, best_threshold):
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Split the data
        left_mask = X[:, best_feature] <= best_threshold

        # Get the left and right data subsets
        left_X, left_y = X[left_mask], y[left_mask]
        right_X, right_y = X[~left_mask], y[~left_mask]

        # Check if the split is valid
        if min(len(left_y), len(right_y)) < self.min_samples_split or len(right_y) < self.min_samples_split:
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Recursively build the left and right subtree
        subtree = Node(
            feature=best_feature,
            threshold=best_threshold,
            left=self._build_tree(left_X, left_y, depth + 1),
            right=self._build_tree(right_X, right_y, depth + 1)
        )

        error_before_pruning = self._calculate_error(X, y, subtree)
        error_after_pruning = self._calculate_error(X, y, Node(class_label=self._get_majority_class(y)))

        if error_after_pruning <= error_before_pruning:
            return Node(class_label=self._get_majority_class(y))

        # Return the subtree
        return subtree

    def _calculate_error(self, X, y, node):
        if node.class_label is not None:
            return np.sum(y != node.class_label) / len(y)

        left_indices = X[:, node.feature] <= node.threshold
        right_indices = ~left_indices

        left_error = self._calculate_error(X[left_indices], y[left_indices], node.left)
        right_error = self._calculate_error(X[right_indices], y[right_indices], node.right)

        left_weight = len(y[left_indices]) / len(y)
        right_weight = len(y[right_indices]) / len(y)

        return left_weight * left_error + right_weight * right_error

    # Get the features to consider for the current node
    def _get_features(self, num_features):
        if self.max_features is None:
            return np.arange(num_features)
        return np.random.choice(np.arange(num_features), size=self.max_features, replace=False)

    # Get the best feature and threshold for the current node
    def _get_split(self, X, y, features):
        best_score = -np.inf
        best_feature, best_threshold = None, None
        for feature in features:
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                score = self._gini_index(y, X[:, feature], threshold)
                if score > best_score:
                    best_score = score
                    best_feature, best_threshold = feature, threshold
        return best_feature, best_threshold

    # Calculate the gini index (Purity of a node)
    def _gini_index(self, y, feature, threshold):
        # Split the data
        left_mask = feature <= threshold
        left_y, right_y = y[left_mask], y[~left_mask]

        # Check if the split is valid
        if len(left_y) < self.min_samples_split or len(right_y) < self.min_samples_split:
            return -np.inf

        # Calculate the probabilitiy of each class label
        p_left = len(left_y) / len(y)
        p_right = len(right_y) / len(y)

        # Calculate the gini index
        gini_left = 1 - np.sum((np.unique(left_y, return_counts=True)[1] / len(left_y)) ** 2)
        gini_right = 1 - np.sum((np.unique(right_y, return_counts=True)[1] / len(right_y)) ** 2)

        # Calculate the weighted gini index
        gini_index = p_left * gini_left + p_right * gini_right

        return gini_index

    # Get the majority class label
    def _get_majority_class(self, y):
        # Get the unique classes and their counts
        unique_classes, class_counts = np.unique(y, return_counts=True)
        return unique_classes[np.argmax(class_counts)]

    # Traverse the decision tree to
    #   get the class label for the current sample
    def _traverse_tree(self, node, x):
        if node.class_label is not None:
            return node.class_label

        if x[node.feature] <= node.threshold:
            return self._traverse_tree(node.left, x)
        return self._traverse_tree(node.right, x)

class RandomForestClassifier:
    def __init__(self, n_estimators=100, max_features=None, max_depth=None, min_samples_split=2, max_processes=25):
        self.n_estimators = n_estimators # if isinstance(n_estimators, int) else 100
        self.max_features = max_features # if isinstance(max_features, int) else None
        self.max_depth = max_depth # if isinstance(max_depth, int) else None
        self.min_samples_split = min_samples_split # if isinstance(min_samples_split, int) else 2
        self.max_processes = max_processes # if isinstance(max_processes, int) else 25
        self.decision_trees = []
        # Used to track the progress of the training
        self.progress = None
        self.start_time = None

    def _build_tree(self, args):
        # Unpack args
        X, y, progress = args

        # Create a bootstrap sample
        bootstrap_indices = np.random.choice(X.shape[0], X.shape[0], replace=True)

        # Create a decision tree
        tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.max_features, min_samples_split=self.min_samples_split)
        tree.fit(X[bootstrap_indices], y[bootstrap_indices])

        # Update progress
        if progress is not None:
            progress.value += 1
            self._print_progress(progress)  # Print progress

        return tree

    def _format_time(self, seconds):
        minutes = seconds // 60
        hours = minutes // 60

        if hours > 0:
            return f'{hours}h {minutes % 60}m {seconds % 60}s'
        if minutes > 0:
            return f'{minutes}m {seconds % 60}s'
        return f'{seconds}s'

    # Print the progress of the training
    def _print_progress(self, progress):
        progress_percentage = (progress.value / self.n_estimators) * 100
        progress_str = f'\rProgress: {progress.value}/{self.n_estimators} trees ({progress_percentage:.2f}%)'

        if self.start_time is not None and progress.value > 0:
            total_time = time.time() - self.start_time
            time_per_tree = total_time / progress.value
            remaining_time = (self.n_estimators - progress.value) * time_per_tree
            progress_str += f', Time taken: {self._format_time(int(total_time))}, Estimated time remaining: {self._format_time(int(remaining_time))}'

        # TODO: Update time remaining every 10 seconds or so (as well as every tree)
        # TODO: Add progress bar?
        print('\r' + ' ' * 100, flush=True, end='') # Clear previous line
        print(progress_str, flush=True, end='')  # Use print with end='' to overwrite previous line

    # Consturct and Train the decision trees
    def fit(self, X, y, show_progress=True):
        self.progress = None
        self.start_time = None

        # TODO: Validate X, y

        if show_progress:
            self.progress = Manager().Value('i', 0)  # Shared progress value
            self.start_time = time.time()  # Start time
            self._print_progress(self.progress)  # Print initial progress

        with Pool(processes=self.max_processes) as pool:
            self.decision_trees = pool.map(self._build_tree, [(X, y, self.progress)] * self.n_estimators)

        if show_progress:
            self._print_progress(self.progress) # Print final progress
            print() # Print new line

    # Predict the class labels
    def predict(self, X):

        # TODO: Validate X

        # Make predictions
        predictions = [tree.predict(X) for tree in self.decision_trees]

        # Get the majority vote for each sample prediction
        return [np.round(np.mean(p)) for p in zip(*predictions)]

if __name__ == '__main__':

    # Load Datasets (1000 elements from each for now)
    bengin = pd.read_csv('./data/benign_data.csv').head(1000)
    malware = pd.read_csv('./data/malware_data.csv').head(1000)

    # Concatenate the dataframes into one dataframe
    data = pd.concat([bengin, malware], ignore_index=True)

    X = data.drop(['Name', 'Parent', 'Malware', 'Code', 'MD5'], axis=1).to_numpy()
    y = data['Malware'].to_numpy()

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y)

    # Initialise Classifier (100 trees for now)
    model = RandomForestClassifier(n_estimators=100, max_features=50, max_processes=20)

    # Fit model with training dataset
    # TODO: Increase accuracy of model (Adjust settings)
    print('Fitting Model')
    model.fit(X_train, y_train)

    # Test the model using test data
    print('Predicting Test Data')
    y_pred = model.predict(X_test)

    # Accuracy score of model
    accuracy = accuracy(y_test, y_pred)
    print("Accuracy:", accuracy)

    if os.path.exists('./build/model.pkl'):
        os.remove('./build/model.pkl')

    # Save the model
    with open('./build/model.pkl', 'wb') as f:
        pickle.dump(model, f)
