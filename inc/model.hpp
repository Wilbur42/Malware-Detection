#include <vector>
#include <unordered_map>
#include <random>
#include <ctime>
#include <iostream>
#include <fstream>
#include <numeric>
#include <limits>
#include <algorithm>


// Decision Tree Node class
class Node {
public:
    Node* left;
    Node* right;
    int featureIndex;
    double threshold;
    int predictedClass;
    bool isLeaf;

    Node() : left(nullptr), right(nullptr), featureIndex(-1), threshold(0.0),
             predictedClass(-1), isLeaf(false) {}
};

// Decision Tree Classifier class
class DecisionTree {
private:
    Node* root;
    int maxDepth;
    int maxFeatures;
    std::vector<std::vector<double>> sortedFeatures;

public:
    DecisionTree(int maxDepth, int maxFeatures, Node* root = nullptr) : maxDepth(maxDepth), maxFeatures(maxFeatures), root(root) {}

    // Train decision tree
    void train(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {
        // Features MUST be sorted for this algorithm to work correctly (ascending order)
        root = buildTree(features, labels, 0);
    }

    // Predict class label for a single instance
    int predict(const std::vector<double>& instance) {
        Node* current = root;
        while (!current->isLeaf) {
            current = (instance[current->featureIndex] <= current->threshold) ? current->left : current->right;
        }
        return current->predictedClass;
    }

    Node* const getRoot() const {
        return this->root;
    }

private:

    // Build decision tree recursively
    Node* buildTree(const std::vector<std::vector<double>>& features, const std::vector<int>& labels, int depth) {
        Node* node = new Node();
        if (depth >= maxDepth || features.empty() || isPure(labels)) {
            node->isLeaf = true;
            if (!labels.empty()) {
                node->predictedClass = getMajorityClass(labels);
            }
        } else {
            std::vector<int> randomFeatureIndex = getRandomFeatures(features[0].size());
            int featureIndex;
            double threshold;
            findBestSplit(features, labels, randomFeatureIndex, featureIndex, threshold);

            node->featureIndex = featureIndex;
            node->threshold = threshold;

            std::vector<std::vector<double>> leftFeatures, rightFeatures;
            std::vector<int> leftLabels, rightLabels;

            // Split data based on threshold
            for (size_t i = 0; i < features.size(); ++i) {
                if (features[i][featureIndex] <= threshold) {
                    leftFeatures.push_back(features[i]);
                    leftLabels.push_back(labels[i]);
                } else {
                    rightFeatures.push_back(features[i]);
                    rightLabels.push_back(labels[i]);
                }
            }

            node->left = buildTree(leftFeatures, leftLabels, depth + 1);
            node->right = buildTree(rightFeatures, rightLabels, depth + 1);
        }
        return node;
    }

    std::vector<int> getRandomFeatures(int num_features) {
        std::vector<int> features(num_features);
        std::iota(features.begin(), features.end(), 0);

        std::random_device rd;
        std::mt19937 g(rd());
        std::shuffle(features.begin(), features.end(), g);

        features.resize(maxFeatures);

        return features;
    }

    // Check if all labels in a set are the same
    bool isPure(const std::vector<int>& labels) {
        if (labels.empty()) {
            return true;
        }
        for (size_t i = 1; i < labels.size(); ++i) {
            if (labels[i] != labels[0]) {
                return false;
            }
        }
        return true;
    }

    // Find the best split point based on Gini impurity
    void findBestSplit(const std::vector<std::vector<double>>& instances, const std::vector<int>& labels,
                    const std::vector<int>& randomFeatureIndices, int& bestFeature, double& bestThreshold) {
        double bestGini = std::numeric_limits<double>::max();

        for (size_t featureIndex : randomFeatureIndices) {
            // Precompute feature values outside the loop
            std::vector<double> featureValues;
            for (const auto& instance : instances) {
                featureValues.push_back(instance[featureIndex]);
            }

            // Sort feature values in ascending order and ensure that labels are sorted accordingly
            std::vector<std::pair<double, int>> sortedFeatureValues;
            for (size_t i = 0; i < featureValues.size(); ++i) {
                sortedFeatureValues.emplace_back(featureValues[i], labels[i]);
            }
            // std::sort(sortedFeatureValues.begin(), sortedFeatureValues.end());

            for (size_t i = 0; i < featureValues.size() - 1; ++i) {
                double threshold = (sortedFeatureValues[i].first + sortedFeatureValues[i + 1].first) / 2.0;

                std::vector<double> leftCount(2, 0), rightCount(2, 0);

                for (size_t j = 0; j < instances.size(); ++j) {
                    const double featureValue = instances[j][featureIndex];
                    const int label = labels[j];

                    (featureValue <= threshold) ? leftCount[label]++ : rightCount[label]++;
                }

                // As the counts are only for binary values, we can calculate the total by adding the two counts
                double leftTotal = leftCount[0] + leftCount[1];
                double rightTotal = rightCount[0] + rightCount[1];

                double numInstances = leftTotal + rightTotal;

                double giniLeft = calculateGini(leftCount, leftTotal);
                double giniRight = calculateGini(rightCount, rightTotal);

                double gini = (leftTotal / numInstances) * giniLeft +
                            (rightTotal / numInstances) * giniRight;

                if (gini < bestGini) {
                    bestGini = gini;
                    bestFeature = featureIndex;
                    bestThreshold = threshold;
                } else {
                    // If gini doesn't improve, stop trying for this featureIndex
                    break;
                }
            }
        }
    }

    double calculateGini(const std::vector<double>& classCounts, double numInstances) {
        if (numInstances == 0) {
            return 0.0;
        }
        double gini = 1.0;
        for (const double& count : classCounts) {
            double classProb = count / numInstances;
            gini -= std::pow(classProb, 2);
        }
        return gini;
    }

    int getMajorityClass(const std::vector<int>& labels) {
        // Important to execution speed
        int classCounts[2] = {0, 0};

        for (const auto& label : labels) {
            classCounts[label]++;
        }

        return (classCounts[0] > classCounts[1]) ? 0 : 1;
    }
};

// Random Forest Classifier class
class RandomForest {
private:
    int numTrees;
    int maxDepth;
    int maxFeatures;
    std::vector<DecisionTree*> trees;

public:
    RandomForest() : numTrees(10), maxDepth(5), maxFeatures(1) {}

    RandomForest(const std::string& filename) : numTrees(10), maxDepth(5), maxFeatures(1) {
        loadModel(filename);
    }

    RandomForest(int numTrees, int maxDepth, int maxFeatures) : numTrees(numTrees), maxDepth(maxDepth), maxFeatures(maxFeatures) {}

    // Train random forest
    void train(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {

        for (int i = 0; i < numTrees; ++i) {
            clock_t startTime = clock();

            DecisionTree* tree = new DecisionTree(maxDepth, maxFeatures);

            std::vector<std::vector<double>> sampledFeatures;
            std::vector<int> sampledLabels;

            // Randomly sample features and labels
            for (size_t j = 0; j < features.size(); ++j) {
                int index = randomInt(0, features.size() - 1);
                sampledFeatures.push_back(features[index]);
                sampledLabels.push_back(labels[index]);
            }

            tree->train(sampledFeatures, sampledLabels);

            trees.push_back(tree);

            clock_t endTime = clock();
            double elapsedTime = double(endTime - startTime) / CLOCKS_PER_SEC * 1000.0;

            std::cout << "Trained tree " << i + 1 << ", " << elapsedTime << " ms" << std::endl;
        }
    }

    int predict(const std::vector<double>& instance) {
        if (trees.empty() || instance.empty()) {
            return -1;
        }

        std::vector<int> classCounts(2, 0);

        for (DecisionTree* tree : trees) {
            int prediction = tree->predict(instance);
            classCounts[prediction]++;
        }

        int predictedClass = (classCounts[0] > classCounts[1]) ? 0 : 1;

        return predictedClass;
    }

    int confidence(const std::vector<double>& instance) {
        if (trees.empty() || instance.empty()) {
            return -1;
        }

        std::vector<int> classCounts(2, 0);

        for (DecisionTree* tree : trees) {
            int prediction = tree->predict(instance);
            classCounts[prediction]++;
        }


        int maxCount = (classCounts[0] > classCounts[1]) ? classCounts[0] : classCounts[1];
        int confidence = (maxCount / trees.size()) * 100;

        return confidence;
    }

    double calculateAccuracy(const std::vector<std::vector<double>>& testFeatures, const std::vector<int>& testLabels) {
        int featureSize = testFeatures.size();
        double correctPredictions = 0;

        for (size_t i = 0; i < featureSize; ++i) {
            if (this->predict(testFeatures[i]) == testLabels[i]) {
                correctPredictions++;
            }
        }

        // std::cout << "Correct predictions: " << correctPredictions << "/" << featureSize << std::endl;

        return correctPredictions / featureSize;
    }

    void saveModel(const std::string& filename) {
        std::stringstream stream;

        // Write the model properties
        stream.write(reinterpret_cast<const char*>(&numTrees), sizeof(numTrees));
        stream.write(reinterpret_cast<const char*>(&maxDepth), sizeof(maxDepth));
        stream.write(reinterpret_cast<const char*>(&maxFeatures), sizeof(maxFeatures));

        // Write each decision tree
        for (const auto& tree : trees) {
            saveDecisionTreeToStream(stream, tree->getRoot());
        }

        std::ofstream file(filename, std::ios::binary);
        if (file) {
            std::string serialisedData = stream.str();
            file.write(serialisedData.c_str(), serialisedData.size());
            file.close();
        } else {
            std::cerr << "Failed to open file" << std::endl;
        }
    }

    void loadModel(const std::string& filename) {
        std::ifstream file(filename, std::ios::binary);
        if (!file) {
            std::cerr << "Failed to open the file" << std::endl;
            return;
        }

        std::stringstream stream;
        stream << file.rdbuf();

        // Read the model properties
        stream.read(reinterpret_cast<char*>(&numTrees), sizeof(numTrees));
        stream.read(reinterpret_cast<char*>(&maxDepth), sizeof(maxDepth));
        stream.read(reinterpret_cast<char*>(&maxFeatures), sizeof(maxFeatures));

        // Load each decision tree
        trees.clear();
        for (int i = 0; i < numTrees; ++i) {
            Node* root = loadDecisionTreeFromStream(stream);
            DecisionTree* tree = new DecisionTree(maxDepth, maxFeatures, root);
            trees.push_back(tree);
        }

        file.close();
    }

private:
    // Helper function to save a decision tree recursively
    void saveDecisionTreeToStream(std::ostream& stream, Node* node) {
        if (!node) {
            return;
        }

        // Save node properties
        stream.write(reinterpret_cast<const char*>(&node->featureIndex), sizeof(node->featureIndex));
        stream.write(reinterpret_cast<const char*>(&node->threshold), sizeof(node->threshold));
        stream.write(reinterpret_cast<const char*>(&node->predictedClass), sizeof(node->predictedClass));
        stream.write(reinterpret_cast<const char*>(&node->isLeaf), sizeof(node->isLeaf));

        // Recursively save left and right subtrees
        saveDecisionTreeToStream(stream, node->left);
        saveDecisionTreeToStream(stream, node->right);
    }

    // Helper function to load a decision tree recursively
    Node* loadDecisionTreeFromStream(std::istream& stream) {
        Node* node = new Node();
        // Load node properties
        stream.read(reinterpret_cast<char*>(&node->featureIndex), sizeof(node->featureIndex));
        stream.read(reinterpret_cast<char*>(&node->threshold), sizeof(node->threshold));
        stream.read(reinterpret_cast<char*>(&node->predictedClass), sizeof(node->predictedClass));
        stream.read(reinterpret_cast<char*>(&node->isLeaf), sizeof(node->isLeaf));

        // Recursively load left and right subtrees if they exist
        if (!node->isLeaf) {
            node->left = loadDecisionTreeFromStream(stream);
            node->right = loadDecisionTreeFromStream(stream);
        }

        return node;
    }
};

// Split data into train and test sets
void trainTestSplit(const std::vector<std::vector<double>>& allFeatures, const std::vector<int>& allLabels,
               std::vector<std::vector<double>>& trainFeatures, std::vector<int>& trainLabels,
               std::vector<std::vector<double>>& testFeatures, std::vector<int>& testLabels,
               double trainRatio = 0.8) {
    if (allFeatures.size() != allLabels.size()) {
        std::cerr << "Mismatch between features and labels!" << std::endl;
        return;
    }

    int numTrainInstances = std::round(allFeatures.size() * trainRatio);

    std::vector<int> indices(allFeatures.size());
    std::iota(indices.begin(), indices.end(), 0);
    std::random_device rd;
    std::mt19937 rng(rd());
    std::shuffle(indices.begin(), indices.end(), rng);

    for (int i = 0; i < numTrainInstances; ++i) {
        trainFeatures.push_back(allFeatures[indices[i]]);
        trainLabels.push_back(allLabels[indices[i]]);
    }

    for (int i = numTrainInstances; i < allFeatures.size(); ++i) {
        testFeatures.push_back(allFeatures[indices[i]]);
        testLabels.push_back(allLabels[indices[i]]);
    }
}