from multiprocessing import Pool
import numpy as np

def train_test_split(data, labels, test_size=0.2, random_seed=None):
    if random_seed is not None:
        np.random.seed(random_seed)

    num_samples = data.shape[0]
    num_test_samples = int(num_samples * test_size)

    # Shuffle the samples
    indices = np.random.permutation(num_samples)

    # Split the samples into training and testing sets
    test_indices = indices[:num_test_samples]
    train_indices = indices[num_test_samples:]

    # Split the data and labels
    train_data = data[train_indices]
    train_labels = labels[train_indices]
    test_data = data[test_indices]
    test_labels = labels[test_indices]

    return train_data, test_data, train_labels, test_labels

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

class Node:
    # A node in the decision tree
    def __init__(self, feature=None, threshold=None, left=None, right=None, class_label=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.class_label = class_label

class DecisionTreeClassifier:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.root = None
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split

    # Train the decision tree
    def fit(self, X, y):
        self.root = self._build_tree(X, y, depth=0)

    # Predict the class labels
    def predict(self, X):
        return [self._traverse_tree(self.root, x) for x in X]

    # Build the decision tree
    def _build_tree(self, X, y, depth):
        # Check the depth of the tree
        if depth == self.max_depth or len(y) < self.min_samples_split:
            # Get the majority class label
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Get indicies of features from the data
        #   (Features - Attributes of each sample)
        features = self._get_features(X.shape[1])

        # Get the best feature and threshold for the current node
        #    (The feature and threshold that best splits the data)
        best_feature, best_threshold = self._get_split(X, y, features)

        # Check if the current node is a leaf node
        if None in (best_feature, best_threshold):
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Split the data
        left_mask = X[:, best_feature] <= best_threshold

        # Get the left and right data subsets
        left_X, left_y = X[left_mask], y[left_mask]
        right_X, right_y = X[~left_mask], y[~left_mask]

        # Check if the split is valid
        if min(len(left_y), len(right_y)) < self.min_samples_split or len(right_y) < self.min_samples_split:
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Recursively build the left and right subtree
        return Node(
            feature=best_feature,
            threshold=best_threshold,
            left=self._build_tree(left_X, left_y, depth+1),
            right=self._build_tree(right_X, right_y, depth+1),
        )

    # Get the features to consider for the current node
    def _get_features(self, num_features):
        return np.arange(num_features)

    # Get the best feature and threshold for the current node
    def _get_split(self, X, y, features):
        best_score = -np.inf
        best_feature, best_threshold = None, None
        for feature in features:
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                score = self._gini_index(y, X[:, feature], threshold)
                if score > best_score:
                    best_score = score
                    best_feature, best_threshold = feature, threshold
        return best_feature, best_threshold

    # Calculate the gini index (Purity of a node)
    def _gini_index(self, y, feature, threshold):
        # Split the data
        left_mask = feature <= threshold
        left_y, right_y = y[left_mask], y[~left_mask]

        # Check if the split is valid
        if len(left_y) < self.min_samples_split or len(right_y) < self.min_samples_split:
            return -np.inf

        # Calculate the probabilitiy of each class label
        p_left = len(left_y) / len(y)
        p_right = len(right_y) / len(y)

        # Calculate the gini index
        gini_left = 1 - np.sum((np.unique(left_y, return_counts=True)[1] / len(left_y)) ** 2)
        gini_right = 1 - np.sum((np.unique(right_y, return_counts=True)[1] / len(right_y)) ** 2)

        # Calculate the weighted gini index
        gini_index = p_left * gini_left + p_right * gini_right

        return gini_index

    # Get the majority class label
    def _get_majority_class(self, y):
        # Get the unique classes and their counts
        unique_classes, class_counts = np.unique(y, return_counts=True)
        return unique_classes[np.argmax(class_counts)]

    # Traverse the decision tree to
    #   get the class label for the current sample
    def _traverse_tree(self, node, x):
        if node.class_label is not None:
            return node.class_label

        if x[node.feature] <= node.threshold:
            return self._traverse_tree(node.left, x)
        return self._traverse_tree(node.right, x)

class RandomForestClassifier:
    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, max_processes=25):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.max_processes = max_processes
        self.decision_trees = []

    def _build_tree(self, args):
        # Unpack args
        X, y = args

        # Create a bootstrap sample
        bootstrap_indices = np.random.choice(X.shape[0], X.shape[0], replace=True)

        # Create a decision tree
        tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
        tree.fit(X[bootstrap_indices], y[bootstrap_indices])

        return tree

    # Consturct and Train the decision trees
    def fit(self, X, y):
        with Pool(processes=self.max_processes) as pool:
            self.decision_trees = pool.map(self._build_tree, [(X, y)] * self.n_estimators)

    # Predict the class labels
    def predict(self, X):
        # Make predictions
        predictions = [tree.predict(X) for tree in self.decision_trees]

        # Get the majority vote for each sample prediction
        return [np.round(np.mean(p)) for p in zip(*predictions)]

if __name__ == '__main__':
    r = RandomForestClassifier()
    x = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])
    y = np.array([0, 0, 1])
    r.fit(x, y)
    p = r.predict(x)

    print(p)