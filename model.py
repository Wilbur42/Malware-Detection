import numpy as np

def train_test_split(data, labels, test_size=0.2, random_seed=None):
    if random_seed is not None:
        np.random.seed(random_seed)

    num_samples = data.shape[0]
    num_test_samples = int(num_samples * test_size)

    # Shuffle the samples
    indices = np.random.permutation(num_samples)

    # Split the samples into training and testing sets
    test_indices = indices[:num_test_samples]
    train_indices = indices[num_test_samples:]

    # Split the data and labels
    train_data = data[train_indices]
    train_labels = labels[train_indices]
    test_data = data[test_indices]
    test_labels = labels[test_indices]

    return train_data, test_data, train_labels, test_labels

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

class Node:
    # A node in the decision tree
    def __init__(self, feature=None, threshold=None, left=None, right=None, class_label=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.class_label = class_label

class DecisionTreeClassifier:
    def __init__(self):
        self.root = None

    # Train the decision tree
    def fit(self, X, y):
        self.root = self._build_tree(X, y, depth=0)

    # Predict the class labels
    def predict(self, X):
        pass

    # Build the decision tree
    def _build_tree(self, X, y, depth):

        # Get indicies of features from the data
        #   (Features - Attributes of each sample)
        features = self._get_features(X.shape[1])

        # Get the best feature and threshold for the current node
        #    (The feature and threshold that best splits the data)
        best_feature, best_threshold = self._get_split(X, y, features)

        # Check if the current node is a leaf node
        if None in (best_feature, best_threshold):
            class_label = self._get_majority_class(y)
            return Node(class_label=class_label)

        # Split the data
        left_mask = X[:, best_feature] <= best_threshold

        # Get the left and right data subsets
        left_X, left_y = X[left_mask], y[left_mask]
        right_X, right_y = X[~left_mask], y[~left_mask]

        # Recursively build the left and right subtree
        return Node(
            feature=best_feature,
            threshold=best_threshold,
            left=self._build_tree(left_X, left_y, depth+1),
            right=self._build_tree(right_X, right_y, depth+1),
        )

    # Get the features to consider for the current node
    def _get_features(self, num_features):
        return np.arange(num_features)

    # Get the best feature and threshold for the current node
    def _get_split(self, X, y, features):
        best_score = -np.inf
        best_feature, best_threshold = None, None
        for feature in features:
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                score = self._gini_index(y, X[:, feature], threshold)
                if score > best_score:
                    best_score = score
                    best_feature, best_threshold = feature, threshold
        return best_feature, best_threshold

    # Calculate the gini index (Purity of a node)
    def _gini_index(self, y, feature, threshold):
        # Split the data
        left_mask = feature <= threshold
        left_y, right_y = y[left_mask], y[~left_mask]

        # Calculate the probabilitiy of each class label
        p_left = len(left_y) / len(y)
        p_right = len(right_y) / len(y)

        # Calculate the gini index
        gini_left = 1 - np.sum((np.unique(left_y, return_counts=True)[1] / len(left_y)) ** 2)
        gini_right = 1 - np.sum((np.unique(right_y, return_counts=True)[1] / len(right_y)) ** 2)

        # Calculate the weighted gini index
        gini_index = p_left * gini_left + p_right * gini_right

        return gini_index

    # Get the majority class label
    def _get_majority_class(self, y):
        # Get the unique classes and their counts
        unique_classes, class_counts = np.unique(y, return_counts=True)
        return unique_classes[np.argmax(class_counts)]

class RandomForestClassifier:
    def __init__(self, n_estimators=100):
        self.n_estimators = n_estimators
        self.decision_trees = []

    # Consturct and Train the decision trees
    def fit(self, X, y):
        for _ in range(self.n_estimators):
            # Create a bootstrap sample
            bootstrap_indices = np.random.choice(X.shape[0], X.shape[0], replace=True)

            # Create a decision tree
            tree = DecisionTreeClassifier()
            tree.fit(X[bootstrap_indices], y[bootstrap_indices])

            # Add the decision tree to the list of decision trees
            self.decision_trees.append(tree)

    # Predict the class labels
    def predict(self, X):
        # Make predictions
        predictions = [tree.predict(X) for tree in self.decision_trees]


if __name__ == '__main__':
    r = RandomForestClassifier()
    x = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])
    y = np.array([0, 0, 1])
    r.fit(x, y)